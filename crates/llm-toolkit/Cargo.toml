[package]
name = "llm-toolkit"
description = "A low-level, unopinionated Rust toolkit for the LLM last mile problem."
keywords = ["llm", "ai", "json", "parser", "prompt"]
categories = ["text-processing", "parsing"]
readme = "README.md"
version.workspace = true
authors.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true

[dependencies]
# Essential Core
serde.workspace = true
serde_json.workspace = true
thiserror.workspace = true
regex.workspace = true
anyhow.workspace = true
minijinja.workspace = true
fuzzy-parser = "0.1"

# Essential Optional
base64.workspace = true
url = { version = "2.5", features = ["serde"] }
mime_guess = "2.0"
schemars = { version = "0.8", optional = true }

# Agent dependencies (optional)
async-trait = { version = "0.1", optional = true }
tracing = { version = "0.1", optional = true }
tracing-log = { version = "0.2", optional = true }
tracing-subscriber = { version = "0.3", features = ["env-filter"], optional = true }
rand = { workspace = true, optional = true }
log = { workspace = true, features = ["std"], optional = true }
tokio = { version = "1", features = ["full"], optional = true }
tokio-util = { version = "0.7", optional = true }
futures = { workspace = true, optional = true }
reqwest = { version = "0.12", features = ["json"], optional = true }

# Ollama API
ollama-rs = { workspace = true, optional = true }

# llama.cpp native binding
llama-cpp-2 = { version = "0.1", optional = true }
hf-hub = { version = "0.4", optional = true }
dirs = { version = "5.0", optional = true }

# Derive macros
llm-toolkit-macros = { workspace = true, optional = true }
quick-xml = { workspace = true, optional = true }

[dev-dependencies]
strum.workspace = true
strum_macros.workspace = true
env_logger = "0.11"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }
tempfile = "3.8"

[features]
default = ["essential"]

# Essential Core - prompt, extract, intent, models (minimal dependencies)
# Use this for structured prompts and LLM output extraction without agent overhead
essential = []

# Derive macros (ToPrompt, ToPromptSet, define_intent)
derive = ["llm-toolkit-macros", "quick-xml"]

# JSON Schema support (for context types)
schema = ["schemars"]

# Agent with CLI spawn (claude, gemini, codex, llama-cpp)
agent = [
    "tokio",
    "tokio-util",
    "futures",
    "derive",
    "async-trait",
    "tracing",
    "tracing-log",
    "tracing-subscriber",
    "rand",
    "log",
    "schema",
]

# API client features (direct HTTP API calls without CLI dependency)
gemini-api = ["agent", "reqwest"]
openai-api = ["agent", "reqwest"]
anthropic-api = ["agent", "reqwest"]
ollama-api = ["agent", "ollama-rs"]
llama-cpp-server = ["agent", "reqwest"]
llama-cpp-native = ["agent", "llama-cpp-2", "hf-hub", "dirs"]
all-apis = ["gemini-api", "openai-api", "anthropic-api", "ollama-api", "llama-cpp-server", "llama-cpp-native"]

# GPU acceleration for llama-cpp-native
cuda = ["llama-cpp-2?/cuda"]
metal = ["llama-cpp-2?/metal"]

[[example]]
name = "derive_prompt"
required-features = ["derive"]

[[example]]
name = "derive_prompt_enum"
required-features = ["derive"]

[[example]]
name = "derive_prompt_format_with"
required-features = ["derive"]

[[example]]
name = "test_define_intent"
required-features = ["derive"]

[[example]]
name = "define_intent_comprehensive"
required-features = ["derive"]

[[example]]
name = "orchestrator_basic"
required-features = ["agent"]

[[example]]
name = "orchestrator_with_mock"
required-features = ["agent"]

[[example]]
name = "check_claude"
required-features = ["agent"]

[[example]]
name = "agent_derive"
required-features = ["agent"]

[[example]]
name = "agent_backend_switching"
required-features = ["agent"]

[[example]]
name = "check_agent_availability"
required-features = ["agent"]

[[example]]
name = "agent_model_selection"
required-features = ["agent"]

[[example]]
name = "agent_custom_backend"
required-features = ["agent"]

[[example]]
name = "orchestrator_get_typed_output"
required-features = ["agent"]

[[example]]
name = "orchestrator_type_marker_production"
required-features = ["agent"]

[[example]]
name = "expandable_basic"
required-features = ["agent"]

[[example]]
name = "agent_expertise_toprompt"
required-features = ["agent"]

[[example]]
name = "agent_with_expertise_full"
required-features = ["agent"]

[[example]]
name = "ollama_basic"
required-features = ["ollama-api"]

[[example]]
name = "llama_cpp_server_basic"
required-features = ["llama-cpp-server"]

[[example]]
name = "llama_cpp_native_basic"
required-features = ["llama-cpp-native"]
